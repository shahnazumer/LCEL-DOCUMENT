{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shahnazumer/LCEL-DOCUMENT/blob/main/Langfuse_Integration_With_langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbSpd5EiZouE"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNyU6IzCZouE"
      },
      "outputs": [],
      "source": [
        "%pip install langfuse langchain langchain_openai --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpE57ujJZouE"
      },
      "source": [
        "Initialize the Langfuse client with your API keys from the project settings in the Langfuse UI and add them to your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEdF-668ZouF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# keys for project from https://cloud.langfuse.com\n",
        "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"\"\n",
        "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"\"\n",
        "os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\" # for EU data region\n",
        "\n",
        "\n",
        "# openai key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "divRadPqZouF"
      },
      "outputs": [],
      "source": [
        "from langfuse.callback import CallbackHandler\n",
        "\n",
        "langfuse_handler = CallbackHandler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FVbg1RWoT8W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7229fa5c-6844-444b-bd2b-2b1a9c019fd4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Tests the SDK connection with the server\n",
        "langfuse_handler.auth_check()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sequential Chain"
      ],
      "metadata": {
        "id": "hDLhVNOczzvI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTagwV_cbFVr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb83e098-4fce-4545-8adc-cb819cbedc43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
            "  warn_deprecated(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/beta_decorator.py:87: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
            "  warn_beta(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        }
      ],
      "source": [
        "# further imports\n",
        "from langchain_openai import OpenAI\n",
        "from langchain.chains import LLMChain, SimpleSequentialChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "llm = OpenAI()\n",
        "template = \"\"\"You are a playwright. Given the title of play, it is your job to write a synopsis for that title.\n",
        "    Title: {title}\n",
        "    Playwright: This is a synopsis for the above play:\"\"\"\n",
        "prompt_template = PromptTemplate(input_variables=[\"title\"], template=template)\n",
        "synopsis_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "template = \"\"\"You are a play critic from the New York Times. Given the synopsis of play, it is your job to write a review for that play.\n",
        "    Play Synopsis:\n",
        "    {synopsis}\n",
        "    Review from a New York Times play critic of the above play:\"\"\"\n",
        "prompt_template = PromptTemplate(input_variables=[\"synopsis\"], template=template)\n",
        "review_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "overall_chain = SimpleSequentialChain(\n",
        "    chains=[synopsis_chain, review_chain],\n",
        ")\n",
        "\n",
        "# invoke\n",
        "review = overall_chain.invoke(\"Tragedy at sunset on the beach\", {\"callbacks\":[langfuse_handler]}) # add the handler to the run method\n",
        "# run\n",
        "review = overall_chain.run(\"Tragedy at sunset on the beach\", callbacks=[langfuse_handler]) # add the handler to the run method"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sequential Chain in Langchain Expression Language (LCEL)"
      ],
      "metadata": {
        "id": "LgcZv5LK0E7F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-HEia6gNoAr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "abe4ef3a-d209-4b6d-f1e9-c72c380d3e31"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Chicago, Illinois se encuentra en Estados Unidos.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "from operator import itemgetter\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema import StrOutputParser\n",
        "\n",
        "langfuse_handler = CallbackHandler()\n",
        "\n",
        "prompt1 = ChatPromptTemplate.from_template(\"what is the city {person} is from?\")\n",
        "prompt2 = ChatPromptTemplate.from_template(\n",
        "    \"what country is the city {city} in? respond in {language}\"\n",
        ")\n",
        "model = ChatOpenAI()\n",
        "chain1 = prompt1 | model | StrOutputParser()\n",
        "chain2 = (\n",
        "    {\"city\": chain1, \"language\": itemgetter(\"language\")}\n",
        "    | prompt2\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "chain2.invoke({\"person\": \"obama\", \"language\": \"spanish\"}, config={\"callbacks\":[langfuse_handler]})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ConversationChain"
      ],
      "metadata": {
        "id": "tchtBoLu0bH-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HXyXC2LGga6"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_openai import OpenAI\n",
        "\n",
        "llm = OpenAI(temperature=0)\n",
        "\n",
        "conversation = ConversationChain(\n",
        "    llm=llm, memory=ConversationBufferMemory()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWRj0qvKHLNE"
      },
      "outputs": [],
      "source": [
        "# Create a callback handler with a session\n",
        "langfuse_handler = CallbackHandler(session_id=\"conversation_chain\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIHmNekVHItt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "cfc481b9-df62-4400-bd41-ba1434e0167f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Hello! It's nice to meet you. I am an AI created by OpenAI. I am constantly learning and improving my abilities through machine learning algorithms. How can I assist you today?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "conversation.predict(input=\"Hi there!\", callbacks=[langfuse_handler])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsAunGSwHkrt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "48ec0abf-f2bf-4b68-c91b-6203b915c8a7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Building great developer tools requires a combination of technical expertise, user research, and continuous iteration. First, it's important to understand the needs and pain points of developers. This can be done through surveys, interviews, and observing their workflows. Then, the tools should be designed with a user-friendly interface and intuitive features. It's also crucial to regularly gather feedback from developers and make improvements based on their suggestions. Additionally, incorporating automation and integrations with other popular tools can greatly enhance the overall experience for developers.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "conversation.predict(input=\"How to build great developer tools?\", callbacks=[langfuse_handler])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8O6hShcHsGe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "80c7dfcb-e933-4ad0-e5bd-32e79a1fc062"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" To build great developer tools, it's important to understand the needs and pain points of developers through research and feedback. The tools should have a user-friendly interface, intuitive features, and incorporate automation and integrations with other popular tools. Continuous iteration and improvement based on user feedback is also crucial.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "conversation.predict(input=\"Summarize your last response\", callbacks=[langfuse_handler])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RetrievalQA"
      ],
      "metadata": {
        "id": "SwlOEgzp0yKD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0CgEPSlEpkC"
      },
      "outputs": [],
      "source": [
        "%pip install unstructured chromadb tiktoken google-search-results python-magic langchainhub --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHDVa-Ssb-KT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "752ff399-c26a-4af0-b727-6bb2295a8118"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'What did the president say about Ketanji Brown Jackson',\n",
              " 'result': ' The president said that he nominated Circuit Court of Appeals Judge Ketanji Brown Jackson to serve on the United States Supreme Court as a successor to retiring Justice Stephen Breyer.'}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "langfuse_handler = CallbackHandler()\n",
        "\n",
        "urls = [\n",
        "    \"https://raw.githubusercontent.com/langfuse/langfuse-docs/main/public/state_of_the_union.txt\",\n",
        "]\n",
        "loader = UnstructuredURLLoader(urls=urls)\n",
        "llm = OpenAI()\n",
        "documents = loader.load()\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "embeddings = OpenAIEmbeddings()\n",
        "docsearch = Chroma.from_documents(texts, embeddings)\n",
        "query = \"What did the president say about Ketanji Brown Jackson\"\n",
        "chain = RetrievalQA.from_chain_type(\n",
        "    llm,\n",
        "    retriever=docsearch.as_retriever(search_kwargs={\"k\": 1}),\n",
        ")\n",
        "\n",
        "chain.invoke(query, config={\"callbacks\":[langfuse_handler]})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "loader=PyPDFLoader('Determinants of LLM-assisted Decision-Making.pdf')\n",
        "docs=loader.load()"
      ],
      "metadata": {
        "id": "6qTW9uGO4Llz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCmI0I20-sbI"
      },
      "source": [
        "### Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ReaHdQOT-S3n"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import AgentExecutor, load_tools, create_openai_functions_agent\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain import hub\n",
        "\n",
        "langfuse_handler = CallbackHandler()\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
        "tools = load_tools([\"serpapi\"])\n",
        "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
        "agent = create_openai_functions_agent(llm, tools, prompt)\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools)\n",
        "\n",
        "agent_executor.invoke({\"input\": \"What is Langfuse?\"}, config={\"callbacks\":[langfuse_handler]})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxQlUOmVPEwz"
      },
      "source": [
        "## Adding scores to traces\n",
        "\n",
        "To add [scores](/docs/scores) to traces created with the Langchain integration, access the traceId via `langfuse_handler.get_trace_id()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PudCopwEPFgh"
      },
      "outputs": [],
      "source": [
        "from langfuse import Langfuse\n",
        "\n",
        "# Trace langchain run via the Langfuse CallbackHandler as shown above\n",
        "\n",
        "# Get id of the last created trace\n",
        "trace_id = langfuse_handler.get_trace_id()\n",
        "\n",
        "# Add score, e.g. via the Python SDK\n",
        "langfuse = Langfuse()\n",
        "trace = langfuse.score(\n",
        "    trace_id=trace_id,\n",
        "    name=\"user-explicit-feedback\",\n",
        "    value=1,\n",
        "    comment=\"I like how personalized the response is\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Pdf reader\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "loader=PyPDFLoader('/content/Determinants of LLM-assisted Decision-Making.pdf')\n",
        "docs=loader.load()"
      ],
      "metadata": {
        "id": "4E72KE1s63ty"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
        "documents=text_splitter.split_documents(docs)\n",
        "documents[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXUvyvwh6732",
        "outputId": "8c5ec8b3-eeda-4b8b-8378-586ce4514142"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='DETERMINANTS OF LLM- ASSISTED DECISION -MAKING\\nEva Eigner and\\n Thorsten Händler\\nFerdinand Porsche Mobile University of Applied Sciences (FERNFH)\\nWiener Neustadt, Austria\\neva.eigner@fernfh.ac.at; thorsten.haendler@fernfh.ac.at\\nABSTRACT\\nDecision-making is a fundamental capability in everyday life. Large Language Models\\n(LLMs) provide multifaceted support in enhancing human decision-making processes.\\nHowever, understanding the influencing factors of LLM-assisted decision-making is crucial\\nfor enabling individuals to utilize LLM-provided advantages and minimize associated risks\\nin order to make more informed and better decisions. This study presents the results of a\\ncomprehensive literature analysis, providing a structural overview and detailed analysis of\\ndeterminants impacting decision-making with LLM support. In particular, we explore the\\neffects of technological aspects of LLMs, including transparency and prompt engineering,', metadata={'source': '/content/Determinants of LLM-assisted Decision-Making.pdf', 'page': 0}),\n",
              " Document(page_content='determinants impacting decision-making with LLM support. In particular, we explore the\\neffects of technological aspects of LLMs, including transparency and prompt engineering,\\npsychological factors such as emotions and decision-making styles, as well as decision-\\nspecific determinants such as task difficulty and accountability. In addition, the impact of the\\ndeterminants on the decision-making process is illustrated via multiple application scenarios.\\nDrawing from our analysis, we develop a dependency framework that systematizes possible\\ninteractions in terms of reciprocal interdependencies between these determinants. Our\\nresearch reveals that, due to the multifaceted interactions with various determinants, factors\\nsuch as trust in or reliance on LLMs, the user’s mental model, and the characteristics\\nof information processing are identified as significant aspects influencing LLM-assisted\\ndecision-making processes. Our findings can be seen as crucial for improving decision', metadata={'source': '/content/Determinants of LLM-assisted Decision-Making.pdf', 'page': 0}),\n",
              " Document(page_content='of information processing are identified as significant aspects influencing LLM-assisted\\ndecision-making processes. Our findings can be seen as crucial for improving decision\\nquality in human-AI collaboration, empowering both users and organizations, and designing\\nmore effective LLM interfaces. Additionally, our work provides a foundation for future\\nempirical investigations on the determinants of decision-making assisted by LLMs.\\nKeywords Decision-making, human-computer interaction, large language models (LLMs), conversational AI,\\npsychological determinants, dependency framework, prompt engineering, over-reliance.\\n1 Introduction\\nEvery day, individuals are confronted with a variety of situations that require decision-making. Consequently,\\nthe ability to make decisions by reflecting relevant information and weighing up available decision options in\\nan efficient way is a critical and fundamental capability [ 121]. For many important decisions, both personal', metadata={'source': '/content/Determinants of LLM-assisted Decision-Making.pdf', 'page': 0}),\n",
              " Document(page_content='an efficient way is a critical and fundamental capability [ 121]. For many important decisions, both personal\\nand professional, individuals seek the advice of experts. While in the past advice commonly has been sought\\nfrom human experts, today advice based on artificial intelligence (AI) is increasingly emerging [ 29,161,117].\\nAI-assisted decision-making is supposed to lead to quicker and improved decision outcomes [ 4]. Hence, its\\nuse in decision-making can be seen as one of the most significant applications of AI [40].\\nLarge Language Models (LLMs) offer versatile assistance in decision-making processes. For instance, their\\nability to process and summarize extensive text data [ 114] enables decision-makers to comprehend key\\ninsights swiftly. Moreover, LLMs are adept at idea generation [ 59] and are capable of generating different\\nsolutions [ 200], enhancing the creation of various alternatives in decision-making. They can also identify', metadata={'source': '/content/Determinants of LLM-assisted Decision-Making.pdf', 'page': 0}),\n",
              " Document(page_content='solutions [ 200], enhancing the creation of various alternatives in decision-making. They can also identify\\npatterns [ 86] and analyze historical data [ 151], thus potentially providing support in the analysis of decision\\nsituations and evaluation of alternatives. Additionally, LLMs demonstrate the capability to adopt personas of\\nvarious characters and engage in social interactions with each other [ 144] and can simulate debates featuringarXiv:2402.17385v1  [cs.AI]  27 Feb 2024', metadata={'source': '/content/Determinants of LLM-assisted Decision-Making.pdf', 'page': 0})]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "langfuse_handler = CallbackHandler()"
      ],
      "metadata": {
        "id": "4lpZedIT7Ehu"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Vector Embedding And Vector Store\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "db = Chroma.from_documents(documents,OpenAIEmbeddings())"
      ],
      "metadata": {
        "id": "YGefB4Wb7wrK"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Who is author of DETERMINANTS OF LLM- ASSISTED DECISION -MAKING\"\n",
        "retireved_results=db.similarity_search(query)"
      ],
      "metadata": {
        "id": "tjp7HvVo7_Rd"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke(query, config={\"callbacks\":[langfuse_handler]})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OSx42vU8im3",
        "outputId": "a2d7fc06-b858-491f-9042-b3ab835629c7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'Who is author of DETERMINANTS OF LLM- ASSISTED DECISION -MAKING',\n",
              " 'result': ' Eva Eigner and Thorsten Händler'}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WWOFhv-y-3BA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}