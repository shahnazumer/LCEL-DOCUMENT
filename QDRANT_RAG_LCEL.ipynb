{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMkwJ3NSbOgQ0GBqfZhnB1X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shahnazumer/LCEL-DOCUMENT/blob/main/QDRANT_RAG_LCEL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "08eYBjBjd7lj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [Detailed](https://python.langchain.com/docs/use_cases/question_answering/)\n",
        "\n",
        "# Retrieval Augmented Generation (RAG)"
      ],
      "metadata": {
        "id": "SzBDACBcD3BX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAG is an AI framework for retrieving facts from an external knowledge base to ground large language models (LLMs) on the most accurate, up-to-date information and to give users insight into LLMs' generative process\n",
        "\n",
        "In the context of this illustration, we will undertake the development of an AI chatbot, encompassing the utilization of LangChain, OpenAI, and Qdrant vector DB. The objective is to construct a chatbot endowed with the capacity to assimilate knowledge from the external environment through Retrieval Augmented Generation (RAG).\n",
        "\n",
        "Upon completing this example, we anticipate having a fully operational chatbot and RAG pipeline capable of engaging in conversations and delivering informative responses based on an integrated knowledge base"
      ],
      "metadata": {
        "id": "6AImeGuXFK9-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prerequisites\n",
        "Before we start building our chatbot, we need to install some Python libraries. Here's a brief overview of what each library does:\n",
        "\n",
        "- **langchain**: This is a library for GenAI. We'll use it to chain together different language models and components for our chatbot.\n",
        "- **openai**: This is the official OpenAI Python client. We'll use it to interact with the OpenAI API and generate responses for our chatbot.\n",
        "- **Qdrant-client**: This is the official Qdrant client. We'll use it to interact with the Qdrant API / HOST and store our chatbot's knowledge base in a vector database.\n",
        "\n",
        "\n",
        "\n",
        "You can install these libraries using pip like so:"
      ],
      "metadata": {
        "id": "1YVvQy6XIYEz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qU8yRTtmDgTd"
      },
      "outputs": [],
      "source": [
        "!pip install langchain==0.0.345 openai qdrant-client langchain-community langchain-core tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Connecting to Qdrant\n",
        "\n",
        "Initially, we will establish a connection between our data and the VectorStore, with Qdrant serving a pivotal role in this process. Qdrant functions as a vector similarity search engine, facilitating efficient retrieval and analysis of information based on vector representations. This integration enhances our ability to perform advanced similarity searches and extract meaningful insights from the connected data within the VectorStore framework\n",
        "\n",
        "\n",
        "I'm using the code from https://github.com/alejandro-ao/qdrant-cloud-app.git"
      ],
      "metadata": {
        "id": "IycwwIvWbxGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We will be relying heavily on the LangChain library to bring together the different components needed for our model;\n",
        "\n",
        "import qdrant_client\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Qdrant\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http import models\n",
        "from qdrant_client import models, QdrantClient\n",
        "from langchain.schema.runnable import RunnablePassthrough"
      ],
      "metadata": {
        "id": "jKvIoTkAMYtS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"]\n"
      ],
      "metadata": {
        "id": "QTLiYfZlNP2Z"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a qdrant instance\n",
        "\n",
        "os.environ['QDRANT_HOST']\n",
        "os.environ[\"QDRANT_API_KEY\"]\n",
        "\n",
        "client = qdrant_client.QdrantClient(\n",
        "    os.getenv(\"QDRANT_HOST\"),\n",
        "    api_key=os.getenv(\"QDRANT_API_KEY\")\n",
        ")"
      ],
      "metadata": {
        "id": "atEuud7KQklT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read the file\n",
        "\n",
        "doc =\"/content/foundation.txt\"\n",
        "data=\"\"\n",
        "\n",
        "with open(doc,'r') as f:\n",
        "    data = f.read()"
      ],
      "metadata": {
        "id": "o1g6-ye2hcB_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split the text into chunks\n",
        "#create a function to return chunks\n",
        "def get_chunks(text):\n",
        "    text_splitter=CharacterTextSplitter(\n",
        "        separator= \"\\n\",\n",
        "        chunk_size=500,\n",
        "        chunk_overlap=100, # second chunk start  character from 800, overlap is used to stop loosing chunk\n",
        "        length_function=len\n",
        "    )\n",
        "\n",
        "    chunks=text_splitter.split_text(text)\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "CJSwWnHZScLu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the chunks for the data\n",
        "texts=get_chunks(data)"
      ],
      "metadata": {
        "id": "otZU0pJxalBq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a new collection\n",
        "\n",
        "vectors_config=models.VectorParams(\n",
        "    # depends on model, we can google dimension. 1536 for openai\n",
        "    # we are using openai embedding, for that size is 1536\n",
        "    size=1536,\n",
        "    distance=models.Distance.COSINE)\n",
        "\n",
        "client.create_collection(\n",
        "    collection_name=\"Isaac Asimov Foundation\",\n",
        "    vectors_config=vectors_config,\n",
        ")"
      ],
      "metadata": {
        "id": "JqdRlFluPay5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a vector store object using langchain\n",
        "\n",
        "# if we want to use any other embedding, we need to change size\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"]\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "vector_store = Qdrant(\n",
        "    client=client,\n",
        "    collection_name=\"Isaac Asimov Foundation\",\n",
        "    embeddings=embeddings,\n",
        ")"
      ],
      "metadata": {
        "id": "UvUZ1huyP-_m"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add chunks to vector store\n",
        "vector_store.add_texts(texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywPTewYSU8Nj",
        "outputId": "ea55cea0-45f7-4820-cdb9-a3165d7cbe17"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1472d37569264ff08b0224a1864ad416',\n",
              " '67bf3e7a87f041598442125ce6579101',\n",
              " 'f1ee28a8d21b41c19edd0b47bfc4f4bb',\n",
              " '41c7d39634ea455493142f7fe4662a78',\n",
              " 'bcb79a4b65eb4a6199c27c106ed219a1']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Advanced Use Case: Retrieval Augmented Generation (RAG)\n",
        "\n",
        "We've built a fully-fledged knowledge base. Now it's time to connect that knowledge base to our chatbot.\n",
        "\n",
        "i have used the code from https://python.langchain.com/docs/expression_language/cookbook/retrieval"
      ],
      "metadata": {
        "id": "2kX5oiopdHja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create retriever & let's try passing a question through to a these vectorstore objects\n",
        "\n",
        "\n",
        "retriever = vector_store.as_retriever()\n",
        "\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "model = ChatOpenAI()"
      ],
      "metadata": {
        "id": "1bBp8PxlU_Sb"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "KuVvLW7RVlTp"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke(\"Who is the creator of psychohistory in the Foundation series?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "x5oz7C3FXnyz",
        "outputId": "6ec048b0-0b5e-4213-da5e-9a3cf4a8ad43"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The creator of psychohistory in the Foundation series is Hari Seldon.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "chain = (\n",
        "    {\n",
        "        \"context\": itemgetter(\"question\") | retriever,\n",
        "        \"question\": itemgetter(\"question\"),\n",
        "    }\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "f_jJz4VJYyh1"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"question\": \"What role do technology and science play in the Foundation series?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "Qh4TKVeiZb1G",
        "outputId": "22d5e867-77da-4f57-ddfe-1c7eca6da608"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Based on the context provided, the role of technology and science in the Foundation series is to explore the idea of predicting and shaping the future through scientific means. The series is centered around the concept of psychohistory, a mathematical discipline that combines history, sociology, and mathematics to predict the future of large populations. Additionally, the series is celebrated for its intellectual depth, intricate world-building, and exploration of the long-term consequences of human actions on a galactic scale.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LCEL RAG with memory\n",
        "\n",
        "I'll be using the code from https://python.langchain.com/docs/expression_language/cookbook/retrieval#with-memory-and-returning-source-documents"
      ],
      "metadata": {
        "id": "xVXEbh0Sgo_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_core.runnables import RunnableLambda, RunnablePassthrough"
      ],
      "metadata": {
        "id": "Gh3Aw3ALZv-T"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.prompt import PromptTemplate\n",
        "\n",
        "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
        "\n",
        "Chat History:\n",
        "{chat_history}\n",
        "Follow Up Input: {question}\n",
        "Standalone question:\"\"\"\n",
        "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)"
      ],
      "metadata": {
        "id": "og2XTQZZm9Pv"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "ANSWER_PROMPT = ChatPromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "DC9bOexxnIVN"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import format_document\n",
        "\n",
        "DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")\n",
        "\n",
        "\n",
        "def _combine_documents(\n",
        "    docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"\n",
        "):\n",
        "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
        "    return document_separator.join(doc_strings)"
      ],
      "metadata": {
        "id": "FyfU9SZ5qxHv"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationBufferMemory(\n",
        "    return_messages=True, output_key=\"answer\", input_key=\"question\"\n",
        ")"
      ],
      "metadata": {
        "id": "fsOSGvDYjc0o"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First we add a step to load memory\n",
        "# This adds a \"memory\" key to the input object\n",
        "\n",
        "loaded_memory = RunnablePassthrough.assign(\n",
        "    chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"),\n",
        ")\n",
        "# Now we calculate the standalone question\n",
        "standalone_question = {\n",
        "        \"standalone_question\": {\n",
        "            \"question\": lambda x: x[\"question\"],\n",
        "            \"chat_history\": lambda x: x[\"chat_history\"],\n",
        "        }\n",
        "        | CONDENSE_QUESTION_PROMPT\n",
        "        | model\n",
        "        | StrOutputParser(),\n",
        "    }\n",
        "\n",
        "# Now we retrieve the documents\n",
        "retrieved_documents = {\n",
        "    \"docs\": itemgetter(\"standalone_question\") | retriever,\n",
        "    \"question\": lambda x: x[\"standalone_question\"],\n",
        "}\n",
        "# Now we construct the inputs for the final prompt\n",
        "final_inputs = {\n",
        "    \"context\": lambda x: _combine_documents(x[\"docs\"]),\n",
        "    \"question\": itemgetter(\"question\"),\n",
        "}\n",
        "# And finally, we do the part that returns the answers\n",
        "answer = {\n",
        "    \"answer\": final_inputs | ANSWER_PROMPT | ChatOpenAI(),\n",
        "    \"docs\": itemgetter(\"docs\"),\n",
        "}\n",
        "# And now we put it all together!\n",
        "final_chain = loaded_memory | standalone_question | retrieved_documents | answer"
      ],
      "metadata": {
        "id": "P0C0Wp2ak0nA"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = {\"question\": \"What is the Seldon Plan and why is it significant??\"}\n",
        "result = final_chain.invoke(inputs)\n",
        "result"
      ],
      "metadata": {
        "id": "AJ6kTsRzluhr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f28bda2a-62f5-4a65-c906-397e7bd9e90c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'answer': AIMessage(content='The significance of the Seldon Plan is that it aims to shorten a predicted dark age of 30,000 years to just 1,000 years by establishing the Foundation, a group of scientists and intellectuals. It is a plan devised by Seldon using psychohistory to guide the course of history and mitigate the chaos and collapse of the Galactic Empire.'),\n",
              " 'docs': [Document(page_content='Seldon has developed psychohistory and foresees the imminent collapse of the Galactic Empire, which has governed \\nthe galaxy for thousands of years. He predicts a dark age lasting 30,000 years but believes that by establishing \\nthe Foundation, a group of scientists and intellectuals, \\nhe can shorten this period of chaos to just 1,000 years.\\nAs the Foundation evolves, it faces numerous challenges, including political turmoil, external threats, and the \\nunpredictable nature of individuals.'),\n",
              "  Document(page_content='Seldon has developed psychohistory and foresees the imminent collapse of the Galactic Empire, which has governed \\nthe galaxy for thousands of years. He predicts a dark age lasting 30,000 years but believes that by establishing \\nthe Foundation, a group of scientists and intellectuals, \\nhe can shorten this period of chaos to just 1,000 years.\\nAs the Foundation evolves, it faces numerous challenges, including political turmoil, external threats, and the \\nunpredictable nature of individuals.'),\n",
              "  Document(page_content='unpredictable nature of individuals. \\nThe series introduces memorable characters such as the \"Mule,\" a mutant with psychic abilities who upends Seldon\\'s \\npredictions, and the mysterious Second Foundation, which works behind the scenes to guide the course of history.\\nThe narrative spans multiple generations, and each book in the series is composed of interconnected short stories or'),\n",
              "  Document(page_content='unpredictable nature of individuals. \\nThe series introduces memorable characters such as the \"Mule,\" a mutant with psychic abilities who upends Seldon\\'s \\npredictions, and the mysterious Second Foundation, which works behind the scenes to guide the course of history.\\nThe narrative spans multiple generations, and each book in the series is composed of interconnected short stories or')]}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Note that the memory does not save automatically\n",
        "# This will be improved in the future\n",
        "# For now you need to save it yourself\n",
        "memory.save_context(inputs, {\"answer\": result[\"answer\"].content})"
      ],
      "metadata": {
        "id": "iyRiMCguncGu"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bglgn_LbdmqG",
        "outputId": "9f04830b-5f0e-4aaa-fb76-53befd0ffb43"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content='What is the Seldon Plan and why is it significant??'),\n",
              "  AIMessage(content='The significance of the Seldon Plan is that it aims to shorten a predicted dark age of 30,000 years to just 1,000 years by establishing the Foundation, a group of scientists and intellectuals. It is a plan devised by Seldon using psychohistory to guide the course of history and mitigate the chaos and collapse of the Galactic Empire.')]}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = {\"question\": \"but what did i ask earlier?\"}\n",
        "result = final_chain.invoke(inputs)\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lv8Cvz0Bdrsx",
        "outputId": "06ddc92f-66df-4a6f-d5e1-37c9e6a69118"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'answer': AIMessage(content='The question you asked earlier is not provided in the given context.'),\n",
              " 'docs': [Document(page_content='Seldon has developed psychohistory and foresees the imminent collapse of the Galactic Empire, which has governed \\nthe galaxy for thousands of years. He predicts a dark age lasting 30,000 years but believes that by establishing \\nthe Foundation, a group of scientists and intellectuals, \\nhe can shorten this period of chaos to just 1,000 years.\\nAs the Foundation evolves, it faces numerous challenges, including political turmoil, external threats, and the \\nunpredictable nature of individuals.'),\n",
              "  Document(page_content='Seldon has developed psychohistory and foresees the imminent collapse of the Galactic Empire, which has governed \\nthe galaxy for thousands of years. He predicts a dark age lasting 30,000 years but believes that by establishing \\nthe Foundation, a group of scientists and intellectuals, \\nhe can shorten this period of chaos to just 1,000 years.\\nAs the Foundation evolves, it faces numerous challenges, including political turmoil, external threats, and the \\nunpredictable nature of individuals.'),\n",
              "  Document(page_content='Isaac Asimov\\'s \"Foundation\" is a classic science fiction series that explores the rise and fall of a galactic empire over thousands of years. \\nThe narrative revolves around the concept of psychohistory, a mathematical discipline that combines history, sociology, and mathematics to \\npredict the future of large populations. Here\\'s a summary of the key aspects of the \"Foundation\" series:\\nThe story begins with mathematician Hari Seldon, who establishes the Foundation on the remote planet Terminus.'),\n",
              "  Document(page_content='Isaac Asimov\\'s \"Foundation\" is a classic science fiction series that explores the rise and fall of a galactic empire over thousands of years. \\nThe narrative revolves around the concept of psychohistory, a mathematical discipline that combines history, sociology, and mathematics to \\npredict the future of large populations. Here\\'s a summary of the key aspects of the \"Foundation\" series:\\nThe story begins with mathematician Hari Seldon, who establishes the Foundation on the remote planet Terminus.')]}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "message_history.add_user_message(message=query)\n",
        "message_history.add_ai_message(message=result[\"answer\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "gefCEs_4eI2r",
        "outputId": "772b4d8a-6931-4af4-a3d0-74f61a92b7c6"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'message_history' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-a58ee28beb1a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmessage_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_user_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmessage_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_ai_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"answer\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'message_history' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #and then we need to write the history manually at the end"
      ],
      "metadata": {
        "id": "56S9RQGLeygF"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OxvwMfyofWl_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}