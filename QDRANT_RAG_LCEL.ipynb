{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP8+2XYxh6cSF0RQsKJANDF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shahnazumer/LCEL-DOCUMENT/blob/main/QDRANT_RAG_LCEL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [Detailed](https://python.langchain.com/docs/use_cases/question_answering/)\n",
        "\n",
        "# Retrieval Augmented Generation (RAG)"
      ],
      "metadata": {
        "id": "SzBDACBcD3BX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAG is an AI framework for retrieving facts from an external knowledge base to ground large language models (LLMs) on the most accurate, up-to-date information and to give users insight into LLMs' generative process\n",
        "\n",
        "In the context of this illustration, we will undertake the development of an AI chatbot, encompassing the utilization of LangChain, OpenAI, and Qdrant vector DB. The objective is to construct a chatbot endowed with the capacity to assimilate knowledge from the external environment through Retrieval Augmented Generation (RAG).\n",
        "\n",
        "Upon completing this example, we anticipate having a fully operational chatbot and RAG pipeline capable of engaging in conversations and delivering informative responses based on an integrated knowledge base"
      ],
      "metadata": {
        "id": "6AImeGuXFK9-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prerequisites\n",
        "Before we start building our chatbot, we need to install some Python libraries. Here's a brief overview of what each library does:\n",
        "\n",
        "- **langchain**: This is a library for GenAI. We'll use it to chain together different language models and components for our chatbot.\n",
        "- **openai**: This is the official OpenAI Python client. We'll use it to interact with the OpenAI API and generate responses for our chatbot.\n",
        "- **Qdrant-client**: This is the official Qdrant client. We'll use it to interact with the Qdrant API / HOST and store our chatbot's knowledge base in a vector database.\n",
        "\n",
        "\n",
        "\n",
        "You can install these libraries using pip like so:"
      ],
      "metadata": {
        "id": "1YVvQy6XIYEz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qU8yRTtmDgTd"
      },
      "outputs": [],
      "source": [
        "!pip install langchain==0.0.345 openai qdrant-client langchain-community langchain-core tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Connecting to Qdrant\n",
        "\n",
        "Initially, we will establish a connection between our data and the VectorStore, with Qdrant serving a pivotal role in this process. Qdrant functions as a vector similarity search engine, facilitating efficient retrieval and analysis of information based on vector representations. This integration enhances our ability to perform advanced similarity searches and extract meaningful insights from the connected data within the VectorStore framework\n",
        "\n",
        "\n",
        "I'm using the code from https://github.com/raen-ai/QD_collections/blob/main/Playground.ipynb"
      ],
      "metadata": {
        "id": "IycwwIvWbxGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We will be relying heavily on the LangChain library to bring together the different components needed for our model;\n",
        "\n",
        "import qdrant_client\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Qdrant\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http import models\n",
        "from qdrant_client import models, QdrantClient\n",
        "from langchain.schema.runnable import RunnablePassthrough"
      ],
      "metadata": {
        "id": "jKvIoTkAMYtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"]"
      ],
      "metadata": {
        "id": "QTLiYfZlNP2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a qdrant instance\n",
        "\n",
        "os.environ['QDRANT_HOST']\n",
        "os.environ[\"QDRANT_API_KEY\"]\n",
        "\n",
        "client = qdrant_client.QdrantClient(\n",
        "    os.getenv(\"QDRANT_HOST\"),\n",
        "    api_key=os.getenv(\"QDRANT_API_KEY\")\n",
        ")"
      ],
      "metadata": {
        "id": "atEuud7KQklT"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read the file\n",
        "\n",
        "doc =\"/content/foundation.txt\"\n",
        "data=\"\"\n",
        "\n",
        "with open(doc,'r') as f:\n",
        "    data = f.read()"
      ],
      "metadata": {
        "id": "o1g6-ye2hcB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split the text into chunks\n",
        "#create a function to return chunks\n",
        "def get_chunks(text):\n",
        "    text_splitter=CharacterTextSplitter(\n",
        "        separator= \"\\n\",\n",
        "        chunk_size=500,\n",
        "        chunk_overlap=100, # second chunk start  character from 800, overlap is used to stop loosing chunk\n",
        "        length_function=len\n",
        "    )\n",
        "\n",
        "    chunks=text_splitter.split_text(text)\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "CJSwWnHZScLu"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the chunks for the data\n",
        "texts=get_chunks(data)"
      ],
      "metadata": {
        "id": "otZU0pJxalBq"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a new collection\n",
        "\n",
        "vectors_config=models.VectorParams(\n",
        "    # depends on model, we can google dimension. 1536 for openai\n",
        "    # we are using openai embedding, for that size is 1536\n",
        "    size=1536,\n",
        "    distance=models.Distance.COSINE)\n",
        "\n",
        "client.create_collection(\n",
        "    collection_name=\"Isaac Asimov Foundation\",\n",
        "    vectors_config=vectors_config,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqdRlFluPay5",
        "outputId": "8dd9c09a-1cd2-4f04-977c-af5571793072"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a vector store object using langchain\n",
        "\n",
        "# if we want to use any other embedding, we need to change size\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"]\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "vector_store = Qdrant(\n",
        "    client=client,\n",
        "    collection_name=\"Isaac Asimov Foundation\",\n",
        "    embeddings=embeddings,\n",
        ")"
      ],
      "metadata": {
        "id": "UvUZ1huyP-_m"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add chunks to vector store\n",
        "vector_store.add_texts(texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywPTewYSU8Nj",
        "outputId": "65fdbdf9-4773-453b-df52-1d1d4aa4d79b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['4d22c83f88344aa591a4ca8a7fd99923',\n",
              " 'f64bc0afbbbc418a97d8ba7a225a4b18',\n",
              " '5e26a9808e8b4f6898f64bd955285477',\n",
              " '2abf9dc397e144ca899266509dde043d',\n",
              " 'ff277314ba10488cb37d6af827fd7fc1']"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Advanced Use Case: Retrieval Augmented Generation (RAG)\n",
        "\n",
        "We've built a fully-fledged knowledge base. Now it's time to connect that knowledge base to our chatbot.\n",
        "\n",
        "i have used the code from https://python.langchain.com/docs/expression_language/cookbook/retrieval"
      ],
      "metadata": {
        "id": "2kX5oiopdHja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create retriever & let's try passing a question through to a these vectorstore objects\n",
        "\n",
        "\n",
        "retriever = vector_store.as_retriever()\n",
        "\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "model = ChatOpenAI()"
      ],
      "metadata": {
        "id": "1bBp8PxlU_Sb"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "KuVvLW7RVlTp"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke(\"Who is the creator of psychohistory in the Foundation series?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "x5oz7C3FXnyz",
        "outputId": "a9443873-71fc-4edb-db8f-43a244a13d7d"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The creator of psychohistory in the Foundation series is Hari Seldon.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "chain = (\n",
        "    {\n",
        "        \"context\": itemgetter(\"question\") | retriever,\n",
        "        \"question\": itemgetter(\"question\"),\n",
        "    }\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "f_jJz4VJYyh1"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"question\": \"What role do technology and science play in the Foundation series?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "Qh4TKVeiZb1G",
        "outputId": "4c863ca8-5562-493f-a88e-0635edb5f9a7"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Technology and science play a significant role in the Foundation series. The series explores the concept of psychohistory, a mathematical discipline that combines history, sociology, and mathematics to predict the future of large populations. The protagonist, Hari Seldon, establishes the Foundation, a group of scientists and intellectuals, to preserve knowledge and guide the future. The Foundation relies on scientific means to shape the future and mitigate the collapse of the Galactic Empire. Additionally, the series emphasizes the power of science and reason, highlighting the interplay between individual actions and historical forces.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LCEL RAG with memory\n",
        "\n",
        "I'll be using the code from https://python.langchain.com/docs/expression_language/cookbook/retrieval#with-memory-and-returning-source-documents"
      ],
      "metadata": {
        "id": "xVXEbh0Sgo_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_core.runnables import RunnableLambda, RunnablePassthrough"
      ],
      "metadata": {
        "id": "Gh3Aw3ALZv-T"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.prompt import PromptTemplate\n",
        "\n",
        "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
        "\n",
        "Chat History:\n",
        "{chat_history}\n",
        "Follow Up Input: {question}\n",
        "Standalone question:\"\"\"\n",
        "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)"
      ],
      "metadata": {
        "id": "og2XTQZZm9Pv"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "ANSWER_PROMPT = ChatPromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "DC9bOexxnIVN"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")\n",
        "\n",
        "\n",
        "def _combine_documents(\n",
        "    docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"\n",
        "):\n",
        "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
        "    return document_separator.join(doc_strings)"
      ],
      "metadata": {
        "id": "FyfU9SZ5qxHv"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationBufferMemory(\n",
        "    return_messages=True, output_key=\"answer\", input_key=\"question\"\n",
        ")"
      ],
      "metadata": {
        "id": "fsOSGvDYjc0o"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First we add a step to load memory\n",
        "# This adds a \"memory\" key to the input object\n",
        "\n",
        "loaded_memory = RunnablePassthrough.assign(\n",
        "    chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"),\n",
        ")\n",
        "# Now we calculate the standalone question\n",
        "standalone_question = {\n",
        "        \"standalone_question\": {\n",
        "            \"question\": lambda x: x[\"question\"],\n",
        "            \"chat_history\": lambda x: x[\"chat_history\"],\n",
        "        }\n",
        "        | CONDENSE_QUESTION_PROMPT\n",
        "        | model\n",
        "        | StrOutputParser(),\n",
        "    }\n",
        "\n",
        "# Now we retrieve the documents\n",
        "retrieved_documents = {\n",
        "    \"docs\": itemgetter(\"standalone_question\") | retriever,\n",
        "    \"question\": lambda x: x[\"standalone_question\"],\n",
        "}\n",
        "# Now we construct the inputs for the final prompt\n",
        "final_inputs = {\n",
        "    \"context\": lambda x: _combine_documents(x[\"docs\"]),\n",
        "    \"question\": itemgetter(\"question\"),\n",
        "}\n",
        "# And finally, we do the part that returns the answers\n",
        "answer = {\n",
        "    \"answer\": final_inputs | ANSWER_PROMPT | ChatOpenAI(),\n",
        "    \"docs\": itemgetter(\"docs\"),\n",
        "}\n",
        "# And now we put it all together!\n",
        "final_chain = loaded_memory | standalone_question | retrieved_documents | answer"
      ],
      "metadata": {
        "id": "P0C0Wp2ak0nA"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = {\"question\": \"What is the Seldon Plan and why is it significant??\"}\n",
        "result = final_chain.invoke(inputs)\n",
        "result"
      ],
      "metadata": {
        "id": "AJ6kTsRzluhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iyRiMCguncGu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}